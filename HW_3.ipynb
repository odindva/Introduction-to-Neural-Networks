{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb9fe0bf",
   "metadata": {},
   "source": [
    "## HW_3\n",
    "### Практическое задание\n",
    "  1. Постройте нейронную сеть(берем простую линейную сеть, которую разбирали на уроке: меняем число слоев, число нейронов , типы активации, тип оптимизатора)  на датасет from sklearn.datasets import load_boston. \n",
    "  2. Измените функцию потерь и метрику для этой задачи. Постройте 10-15 вариантов и сведите результаты их работы в таблицу  Опишите, какого результата вы добились от нейросети? Что помогло вам улучшить ее точность?\n",
    "\n",
    "  3. Поработайте с документацией TensorFlow 2. Найти 2-3 полезные команды TensorFlow, не разобранные на уроке (полезные для Вас).\n",
    "\n",
    "  1-2. (*)  Попробуйте обучить нейронную сеть на TensorFlow 2 на датасете imdb_reviews .Опишите, какого результата вы добились от нейросети? Что помогло вам улучшить ее точность?\n",
    "    \n",
    "    \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d8f427f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from tensorflow.keras.datasets import boston_housing\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0f44cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "TensorFlow 2.0 Hello World\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "msg = tf.constant('TensorFlow 2.0 Hello World')\n",
    "tf.print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "945094fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train),(X_test, y_test)=boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca02737b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73c9d413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 13) (102, 13)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d6ac621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.27224633, -0.48361547, -0.43576161, ...,  1.14850044,\n",
       "         0.44807713,  0.8252202 ],\n",
       "       [-0.40342651,  2.99178419, -1.33391162, ..., -1.71818909,\n",
       "         0.43190599, -1.32920239],\n",
       "       [ 0.1249402 , -0.48361547,  1.0283258 , ...,  0.78447637,\n",
       "         0.22061726, -1.30850006],\n",
       "       ...,\n",
       "       [-0.40202987,  0.99079651, -0.7415148 , ..., -0.71712291,\n",
       "         0.07943894, -0.67776904],\n",
       "       [-0.17292018, -0.48361547,  1.24588095, ..., -1.71818909,\n",
       "        -0.98764362,  0.42083466],\n",
       "       [-0.40422614,  2.04394792, -1.20161456, ..., -1.30866202,\n",
       "         0.23317118, -1.15392266]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba2198a",
   "metadata": {},
   "source": [
    "Так как знаю не все лоссы и метрики, то просто добавил методом исключения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69dce6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = [\n",
    "    tf.keras.losses.Huber,\n",
    "    tf.keras.losses.KLDivergence,\n",
    "    tf.keras.losses.LogCosh,\n",
    "    tf.keras.losses.MeanAbsoluteError,\n",
    "    tf.keras.losses.MeanAbsolutePercentageError,\n",
    "    tf.keras.losses.MeanSquaredError,\n",
    "    tf.keras.losses.MeanSquaredLogarithmicError,\n",
    "]\n",
    "metrics = [\n",
    "    tf.keras.metrics.CosineSimilarity,\n",
    "    tf.keras.metrics.Hinge,\n",
    "    tf.keras.metrics.KLDivergence,\n",
    "    tf.keras.metrics.LogCoshError,\n",
    "    tf.keras.metrics.MeanAbsoluteError,\n",
    "    tf.keras.metrics.MeanAbsolutePercentageError,\n",
    "    tf.keras.metrics.MeanSquaredError,\n",
    "    tf.keras.metrics.MeanSquaredLogarithmicError,\n",
    "    tf.keras.metrics.Poisson,\n",
    "    tf.keras.metrics.RootMeanSquaredError,\n",
    "    tf.keras.metrics.SquaredHinge\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0da8dcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55794d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(\n",
    "                              monitor=\"val_loss\",\n",
    "                              min_delta=0.0001,\n",
    "                              patience=3,\n",
    "                              verbose=1,\n",
    "                              mode=\"auto\",\n",
    "                              baseline=None,\n",
    "                              restore_best_weights=True,\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c327d2f",
   "metadata": {},
   "source": [
    "сравнивать модели будем по знакомой и понятной метрике $R^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4b7759a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Huber_CosineSimilarity\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00018: early stopping\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.8226 - cosine_similarity: 1.0000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.5886 - cosine_similarity: 1.0000\n",
      "\n",
      "Huber_Hinge\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00022: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.7889 - hinge: 0.0000e+00\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.4897 - hinge: 0.0000e+00\n",
      "\n",
      "Huber_KLDivergence\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00019: early stopping\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.8734 - kullback_leibler_divergence: 0.0000e+00\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.5261 - kullback_leibler_divergence: 0.0000e+00\n",
      "\n",
      "Huber_LogCoshError\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00023: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.7906 - logcosh: 1.6827\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.5319 - logcosh: 2.4016\n",
      "\n",
      "Huber_MeanAbsoluteError\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00033: early stopping\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.6799 - mean_absolute_error: 2.1060\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.4148 - mean_absolute_error: 2.8515\n",
      "\n",
      "Huber_MeanAbsolutePercentageError\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00024: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.7052 - mean_absolute_percentage_error: 9.9646\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.6162 - mean_absolute_percentage_error: 15.7023\n",
      "\n",
      "Huber_MeanSquaredError\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00031: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.6107 - mean_squared_error: 10.4714\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.4750 - mean_squared_error: 18.8329\n",
      "\n",
      "Huber_MeanSquaredLogarithmicError\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00017: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.9026 - mean_squared_logarithmic_error: 0.0234\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.4313 - mean_squared_logarithmic_error: 0.0408\n",
      "\n",
      "Huber_Poisson\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00021: early stopping\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.8730 - poisson: -48.7189\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.5140 - poisson: -50.7367\n",
      "\n",
      "Huber_RootMeanSquaredError\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00018: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.9515 - root_mean_squared_error: 3.7297\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.6139 - root_mean_squared_error: 4.3099\n",
      "\n",
      "Huber_SquaredHinge\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00033: early stopping\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.6540 - squared_hinge: 0.0000e+00\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.5721 - squared_hinge: 0.0000e+00\n",
      "\n",
      "KLDivergence_CosineSimilarity\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0012 - cosine_similarity: 1.0000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 8.2830e-04 - cosine_similarity: 1.0000\n",
      "\n",
      "KLDivergence_Hinge\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0011 - hinge: 0.0000e+00\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 6.4783e-04 - hinge: 0.0000e+00\n",
      "\n",
      "KLDivergence_KLDivergence\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00005: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - kullback_leibler_divergence: 0.0000e+00\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 4.6774e-04 - kullback_leibler_divergence: 4.6774e-04\n",
      "\n",
      "KLDivergence_LogCoshError\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.2590e-05 - logcosh: 19.4814\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 1.2445e-04 - logcosh: 20.1877\n",
      "\n",
      "KLDivergence_MeanAbsoluteError\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 3.3644e-04 - mean_absolute_error: 20.5171\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.0023 - mean_absolute_error: 21.2610\n",
      "\n",
      "KLDivergence_MeanAbsolutePercentageError\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0056 - mean_absolute_percentage_error: 90.2899\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.0051 - mean_absolute_percentage_error: 90.3760\n",
      "\n",
      "KLDivergence_MeanSquaredError\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.3742e-05 - mean_squared_error: 499.9783\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 3.2984e-04 - mean_squared_error: 524.8524\n",
      "\n",
      "KLDivergence_MeanSquaredLogarithmicError\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00021: early stopping\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 8.2067e-05 - mean_squared_logarithmic_error: 4.3481\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - mean_squared_logarithmic_error: 4.5332\n",
      "\n",
      "KLDivergence_Poisson\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 6.6300e-04 - poisson: -9.5508\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - poisson: -9.9128\n",
      "\n",
      "KLDivergence_RootMeanSquaredError\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 6.7214e-04 - root_mean_squared_error: 22.4338\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.9212e-04 - root_mean_squared_error: 23.0558\n",
      "\n",
      "KLDivergence_SquaredHinge\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - squared_hinge: 0.0000e+00\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - squared_hinge: 0.0000e+00\n",
      "\n",
      "LogCosh_CosineSimilarity\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.9017 - cosine_similarity: 1.0000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.5382 - cosine_similarity: 1.0000\n",
      "\n",
      "LogCosh_Hinge\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00034: early stopping\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4563 - hinge: 0.0000e+00\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.2712 - hinge: 0.0000e+00\n",
      "\n",
      "LogCosh_KLDivergence\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00027: early stopping\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.5866 - kullback_leibler_divergence: 0.0000e+00\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.4755 - kullback_leibler_divergence: 0.0000e+00\n",
      "\n",
      "LogCosh_LogCoshError\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.8390 - logcosh: 1.8390\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.4581 - logcosh: 2.4581\n",
      "\n",
      "LogCosh_MeanAbsoluteError\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00027: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.6381 - mean_absolute_error: 2.1723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 5ms/step - loss: 2.2914 - mean_absolute_error: 2.8697\n",
      "\n",
      "LogCosh_MeanAbsolutePercentageError\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.7318 - mean_absolute_percentage_error: 10.7651\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.3287 - mean_absolute_percentage_error: 15.1403\n",
      "\n",
      "LogCosh_MeanSquaredError\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00026: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.6259 - mean_squared_error: 12.2211\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.2361 - mean_squared_error: 16.5425\n",
      "\n",
      "LogCosh_MeanSquaredLogarithmicError\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 01514: early stopping\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 5.8759 - mean_squared_logarithmic_error: 0.1452\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 6.0665 - mean_squared_logarithmic_error: 0.1672\n",
      "\n",
      "LogCosh_Poisson\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00025: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.5976 - poisson: -48.7541\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.2507 - poisson: -50.7379\n",
      "\n",
      "LogCosh_RootMeanSquaredError\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00021: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.6289 - root_mean_squared_error: 3.4923\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.2503 - root_mean_squared_error: 4.1682\n",
      "\n",
      "LogCosh_SquaredHinge\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00033: early stopping\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4989 - squared_hinge: 0.0000e+00\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.3431 - squared_hinge: 0.0000e+00\n",
      "\n",
      "MeanAbsoluteError_CosineSimilarity\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00017: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.4001 - cosine_similarity: 1.0000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 3.0741 - cosine_similarity: 1.0000\n",
      "\n",
      "MeanAbsoluteError_Hinge\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00026: early stopping\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.1413 - hinge: 0.0000e+00\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.7897 - hinge: 0.0000e+00\n",
      "\n",
      "MeanAbsoluteError_KLDivergence\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00022: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.2922 - kullback_leibler_divergence: 0.0000e+00\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.8603 - kullback_leibler_divergence: 0.0000e+00\n",
      "\n",
      "MeanAbsoluteError_LogCoshError\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00020: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.4056 - logcosh: 1.8569\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.2110 - logcosh: 2.6094\n",
      "\n",
      "MeanAbsoluteError_MeanAbsoluteError\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00019: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.3396 - mean_absolute_error: 2.3396\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.0850 - mean_absolute_error: 3.0850\n",
      "\n",
      "MeanAbsoluteError_MeanAbsolutePercentageError\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00026: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.1970 - mean_absolute_percentage_error: 9.9700\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.9134 - mean_absolute_percentage_error: 14.6109\n",
      "\n",
      "MeanAbsoluteError_MeanSquaredError\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00029: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.1320 - mean_squared_error: 12.7259\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 3.0083 - mean_squared_error: 22.2178\n",
      "\n",
      "MeanAbsoluteError_MeanSquaredLogarithmicError\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00027: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.2408 - mean_squared_logarithmic_error: 0.0216\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 3.0636 - mean_squared_logarithmic_error: 0.0431\n",
      "\n",
      "MeanAbsoluteError_Poisson\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.4336 - poisson: -48.6549\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 3.1405 - poisson: -50.6697\n",
      "\n",
      "MeanAbsoluteError_RootMeanSquaredError\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.5685 - root_mean_squared_error: 4.0772\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.0885 - root_mean_squared_error: 4.2831\n",
      "\n",
      "MeanAbsoluteError_SquaredHinge\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00019: early stopping\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2793 - squared_hinge: 0.0000e+00\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 3.0325 - squared_hinge: 0.0000e+00\n",
      "\n",
      "MeanAbsolutePercentageError_CosineSimilarity\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00022: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 10.6379 - cosine_similarity: 1.0000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 15.3229 - cosine_similarity: 1.0000\n",
      "\n",
      "MeanAbsolutePercentageError_Hinge\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00018: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 10.3592 - hinge: 0.0000e+00\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 14.2479 - hinge: 0.0000e+00\n",
      "\n",
      "MeanAbsolutePercentageError_KLDivergence\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00020: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 10.7518 - kullback_leibler_divergence: 0.0000e+00\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 14.9976 - kullback_leibler_divergence: 0.0000e+00\n",
      "\n",
      "MeanAbsolutePercentageError_LogCoshError\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00031: early stopping\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 9.6895 - logcosh: 1.6130\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 13.9075 - logcosh: 2.1899\n",
      "\n",
      "MeanAbsolutePercentageError_MeanAbsoluteError\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00020: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 10.3016 - mean_absolute_error: 2.2661\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 14.3147 - mean_absolute_error: 2.8123\n",
      "\n",
      "MeanAbsolutePercentageError_MeanAbsolutePercentageError\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00021: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 10.5630 - mean_absolute_percentage_error: 10.5630\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 14.2574 - mean_absolute_percentage_error: 14.2574\n",
      "\n",
      "MeanAbsolutePercentageError_MeanSquaredError\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00022: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 10.3220 - mean_squared_error: 15.8770\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 13.7454 - mean_squared_error: 17.4369\n",
      "\n",
      "MeanAbsolutePercentageError_MeanSquaredLogarithmicError\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00027: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 10.9166 - mean_squared_logarithmic_error: 0.0283\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 15.2899 - mean_squared_logarithmic_error: 0.0429\n",
      "\n",
      "MeanAbsolutePercentageError_Poisson\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00024: early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 3ms/step - loss: 10.0791 - poisson: -48.6824\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 14.4320 - poisson: -50.7528\n",
      "\n",
      "MeanAbsolutePercentageError_RootMeanSquaredError\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00032: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 10.6326 - root_mean_squared_error: 3.7261\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 14.2337 - root_mean_squared_error: 4.0475\n",
      "\n",
      "MeanAbsolutePercentageError_SquaredHinge\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00024: early stopping\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 10.0721 - squared_hinge: 0.0000e+00\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 14.7086 - squared_hinge: 0.0000e+00\n",
      "\n",
      "MeanSquaredError_CosineSimilarity\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00032: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 8.8969 - cosine_similarity: 1.0000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 24.0934 - cosine_similarity: 1.0000\n",
      "\n",
      "MeanSquaredError_Hinge\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00025: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 10.1403 - hinge: 0.0000e+00\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 25.6008 - hinge: 0.0000e+00\n",
      "\n",
      "MeanSquaredError_KLDivergence\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00023: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 10.7598 - kullback_leibler_divergence: 0.0000e+00\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 25.2845 - kullback_leibler_divergence: 0.0000e+00\n",
      "\n",
      "MeanSquaredError_LogCoshError\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 11.7304 - logcosh: 1.8956\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 21.9765 - logcosh: 2.7516\n",
      "\n",
      "MeanSquaredError_MeanAbsoluteError\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00028: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 10.1727 - mean_absolute_error: 2.2286\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 23.6930 - mean_absolute_error: 3.2426\n",
      "\n",
      "MeanSquaredError_MeanAbsolutePercentageError\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00021: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 11.9462 - mean_absolute_percentage_error: 11.6084\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 27.3399 - mean_absolute_percentage_error: 17.3385\n",
      "\n",
      "MeanSquaredError_MeanSquaredError\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00025: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 10.4337 - mean_squared_error: 10.4337\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 24.2018 - mean_squared_error: 24.2018\n",
      "\n",
      "MeanSquaredError_MeanSquaredLogarithmicError\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00021: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 11.2494 - mean_squared_logarithmic_error: 0.0224\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 26.7038 - mean_squared_logarithmic_error: 0.0481\n",
      "\n",
      "MeanSquaredError_Poisson\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00023: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 10.7967 - poisson: -48.7503\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 24.1540 - poisson: -50.6497\n",
      "\n",
      "MeanSquaredError_RootMeanSquaredError\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00029: early stopping\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 9.4534 - root_mean_squared_error: 3.0746\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 24.7450 - root_mean_squared_error: 4.9744\n",
      "\n",
      "MeanSquaredError_SquaredHinge\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00024: early stopping\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 10.9433 - squared_hinge: 0.0000e+00\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 23.3675 - squared_hinge: 0.0000e+00\n",
      "\n",
      "MeanSquaredLogarithmicError_CosineSimilarity\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00042: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0230 - cosine_similarity: 1.0000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.0389 - cosine_similarity: 1.0000\n",
      "\n",
      "MeanSquaredLogarithmicError_Hinge\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00069: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0196 - hinge: 0.0000e+00\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.0379 - hinge: 0.0000e+00\n",
      "\n",
      "MeanSquaredLogarithmicError_KLDivergence\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00042: early stopping\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0234 - kullback_leibler_divergence: 0.0000e+00\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.0419 - kullback_leibler_divergence: 0.0000e+00\n",
      "\n",
      "MeanSquaredLogarithmicError_LogCoshError\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00055: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0203 - logcosh: 1.8808\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0415 - logcosh: 2.6807\n",
      "\n",
      "MeanSquaredLogarithmicError_MeanAbsoluteError\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00048: early stopping\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0206 - mean_absolute_error: 2.4210\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0422 - mean_absolute_error: 3.0485\n",
      "\n",
      "MeanSquaredLogarithmicError_MeanAbsolutePercentageError\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00040: early stopping\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0240 - mean_absolute_percentage_error: 12.4100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.0463 - mean_absolute_percentage_error: 17.2927\n",
      "\n",
      "MeanSquaredLogarithmicError_MeanSquaredError\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00004: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 9.6347 - mean_squared_error: 604.0202\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 9.8067 - mean_squared_error: 635.5961\n",
      "\n",
      "MeanSquaredLogarithmicError_MeanSquaredLogarithmicError\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00004: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 9.6347 - mean_squared_logarithmic_error: 9.6347\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 9.8067 - mean_squared_logarithmic_error: 9.8067\n",
      "\n",
      "MeanSquaredLogarithmicError_Poisson\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00043: early stopping\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0218 - poisson: -48.7185\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.0430 - poisson: -50.7043\n",
      "\n",
      "MeanSquaredLogarithmicError_RootMeanSquaredError\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00048: early stopping\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0211 - root_mean_squared_error: 3.5492\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.0454 - root_mean_squared_error: 4.6831\n",
      "\n",
      "MeanSquaredLogarithmicError_SquaredHinge\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00048: early stopping\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0210 - squared_hinge: 0.0000e+00\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.0367 - squared_hinge: 0.0000e+00\n",
      "\n",
      "CPU times: total: 9min 7s\n",
      "Wall time: 7min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for current_loss in losses:\n",
    "    for current_metric in metrics:\n",
    "        name = f'{str(current_loss).split(\".\")[-1][:-2]}_{str(current_metric).split(\".\")[-1][:-2]}'\n",
    "        print(name)\n",
    "        model = Sequential([\n",
    "            Dense(100, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "            Dense(50, activation='relu'),\n",
    "            Dense(25, activation='relu'),\n",
    "            Dense(10, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss=[current_loss()],\n",
    "            metrics=[current_metric()]\n",
    "        )\n",
    "        history = model.fit(X_train, y_train,\n",
    "                      epochs=10000,\n",
    "                      batch_size=20, \n",
    "                      validation_split=0.2,\n",
    "                      verbose=0,\n",
    "                      callbacks=[callback],\n",
    "                  )\n",
    "        train_metrics = model.evaluate(X_train, y_train)\n",
    "        test_metrics = model.evaluate(X_test, y_test)\n",
    "        pred_train = model.predict(X_train)\n",
    "        train_r2 = r2_score(y_train, pred_train)\n",
    "        pred_test = model.predict(X_test)\n",
    "        test_r2 = r2_score(y_test, pred_test)\n",
    "        \n",
    "        models.append({\n",
    "            'name': name,\n",
    "            'model': model,\n",
    "            'history': history,\n",
    "            'train_loss': train_metrics[0],\n",
    "            'test_loss': test_metrics[0],\n",
    "            'train_metric': train_metrics[1],\n",
    "            'test_metric': test_metrics[1],\n",
    "            'train_r2': train_r2,\n",
    "            'test_r2': test_r2,\n",
    "        })\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d75ad26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>train_metric</th>\n",
       "      <th>test_metric</th>\n",
       "      <th>train_r2</th>\n",
       "      <th>test_r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Huber_CosineSimilarity</td>\n",
       "      <td>1.822563</td>\n",
       "      <td>2.588642</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.850980</td>\n",
       "      <td>0.752793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Huber_Hinge</td>\n",
       "      <td>1.788935</td>\n",
       "      <td>2.489674</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.847274</td>\n",
       "      <td>0.781944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Huber_KLDivergence</td>\n",
       "      <td>1.873441</td>\n",
       "      <td>2.526053</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.837377</td>\n",
       "      <td>0.762149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Huber_LogCoshError</td>\n",
       "      <td>1.790627</td>\n",
       "      <td>2.531869</td>\n",
       "      <td>1.682698</td>\n",
       "      <td>2.401638</td>\n",
       "      <td>0.846779</td>\n",
       "      <td>0.771626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Huber_MeanAbsoluteError</td>\n",
       "      <td>1.679937</td>\n",
       "      <td>2.414753</td>\n",
       "      <td>2.106008</td>\n",
       "      <td>2.851494</td>\n",
       "      <td>0.866358</td>\n",
       "      <td>0.779942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>MeanSquaredLogarithmicError_MeanSquaredError</td>\n",
       "      <td>9.634681</td>\n",
       "      <td>9.806747</td>\n",
       "      <td>604.020203</td>\n",
       "      <td>635.596130</td>\n",
       "      <td>-6.137841</td>\n",
       "      <td>-6.635353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>MeanSquaredLogarithmicError_MeanSquaredLogarit...</td>\n",
       "      <td>9.634681</td>\n",
       "      <td>9.806747</td>\n",
       "      <td>9.634681</td>\n",
       "      <td>9.806747</td>\n",
       "      <td>-6.027299</td>\n",
       "      <td>-6.510259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>MeanSquaredLogarithmicError_Poisson</td>\n",
       "      <td>0.021758</td>\n",
       "      <td>0.042951</td>\n",
       "      <td>-48.718491</td>\n",
       "      <td>-50.704308</td>\n",
       "      <td>0.839641</td>\n",
       "      <td>0.762187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>MeanSquaredLogarithmicError_RootMeanSquaredError</td>\n",
       "      <td>0.021062</td>\n",
       "      <td>0.045442</td>\n",
       "      <td>3.549192</td>\n",
       "      <td>4.683145</td>\n",
       "      <td>0.851141</td>\n",
       "      <td>0.736535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>MeanSquaredLogarithmicError_SquaredHinge</td>\n",
       "      <td>0.020969</td>\n",
       "      <td>0.036704</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.849373</td>\n",
       "      <td>0.780268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>77 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 name  train_loss  test_loss  \\\n",
       "0                              Huber_CosineSimilarity    1.822563   2.588642   \n",
       "1                                         Huber_Hinge    1.788935   2.489674   \n",
       "2                                  Huber_KLDivergence    1.873441   2.526053   \n",
       "3                                  Huber_LogCoshError    1.790627   2.531869   \n",
       "4                             Huber_MeanAbsoluteError    1.679937   2.414753   \n",
       "..                                                ...         ...        ...   \n",
       "72       MeanSquaredLogarithmicError_MeanSquaredError    9.634681   9.806747   \n",
       "73  MeanSquaredLogarithmicError_MeanSquaredLogarit...    9.634681   9.806747   \n",
       "74                MeanSquaredLogarithmicError_Poisson    0.021758   0.042951   \n",
       "75   MeanSquaredLogarithmicError_RootMeanSquaredError    0.021062   0.045442   \n",
       "76           MeanSquaredLogarithmicError_SquaredHinge    0.020969   0.036704   \n",
       "\n",
       "    train_metric  test_metric  train_r2   test_r2  \n",
       "0       1.000000     1.000000  0.850980  0.752793  \n",
       "1       0.000000     0.000000  0.847274  0.781944  \n",
       "2       0.000000     0.000000  0.837377  0.762149  \n",
       "3       1.682698     2.401638  0.846779  0.771626  \n",
       "4       2.106008     2.851494  0.866358  0.779942  \n",
       "..           ...          ...       ...       ...  \n",
       "72    604.020203   635.596130 -6.137841 -6.635353  \n",
       "73      9.634681     9.806747 -6.027299 -6.510259  \n",
       "74    -48.718491   -50.704308  0.839641  0.762187  \n",
       "75      3.549192     4.683145  0.851141  0.736535  \n",
       "76      0.000000     0.000000  0.849373  0.780268  \n",
       "\n",
       "[77 rows x 7 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(models).drop(['model', 'history'],axis=1)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fe90fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>train_metric</th>\n",
       "      <th>test_metric</th>\n",
       "      <th>train_r2</th>\n",
       "      <th>test_r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>MeanAbsolutePercentageError_MeanAbsoluteError</td>\n",
       "      <td>10.301604</td>\n",
       "      <td>14.314724</td>\n",
       "      <td>2.266057</td>\n",
       "      <td>2.812323</td>\n",
       "      <td>0.818855</td>\n",
       "      <td>0.812048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>MeanAbsolutePercentageError_RootMeanSquaredError</td>\n",
       "      <td>10.632647</td>\n",
       "      <td>14.233661</td>\n",
       "      <td>3.726139</td>\n",
       "      <td>4.047540</td>\n",
       "      <td>0.835928</td>\n",
       "      <td>0.803198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogCosh_MeanSquaredError</td>\n",
       "      <td>1.625854</td>\n",
       "      <td>2.236084</td>\n",
       "      <td>12.221090</td>\n",
       "      <td>16.542501</td>\n",
       "      <td>0.855581</td>\n",
       "      <td>0.801277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Huber_MeanSquaredLogarithmicError</td>\n",
       "      <td>1.902591</td>\n",
       "      <td>2.431320</td>\n",
       "      <td>0.023448</td>\n",
       "      <td>0.040783</td>\n",
       "      <td>0.821507</td>\n",
       "      <td>0.800495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>MeanAbsoluteError_Hinge</td>\n",
       "      <td>2.141349</td>\n",
       "      <td>2.789681</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.837433</td>\n",
       "      <td>0.799163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>MeanAbsolutePercentageError_Poisson</td>\n",
       "      <td>10.079053</td>\n",
       "      <td>14.431956</td>\n",
       "      <td>-48.682350</td>\n",
       "      <td>-50.752823</td>\n",
       "      <td>0.817363</td>\n",
       "      <td>0.797132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogCosh_MeanAbsolutePercentageError</td>\n",
       "      <td>1.731798</td>\n",
       "      <td>2.328740</td>\n",
       "      <td>10.765066</td>\n",
       "      <td>15.140324</td>\n",
       "      <td>0.834882</td>\n",
       "      <td>0.791863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>LogCosh_RootMeanSquaredError</td>\n",
       "      <td>1.628946</td>\n",
       "      <td>2.250333</td>\n",
       "      <td>3.492333</td>\n",
       "      <td>4.168153</td>\n",
       "      <td>0.855873</td>\n",
       "      <td>0.791294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>MeanAbsolutePercentageError_MeanSquaredError</td>\n",
       "      <td>10.322030</td>\n",
       "      <td>13.745417</td>\n",
       "      <td>15.876963</td>\n",
       "      <td>17.436861</td>\n",
       "      <td>0.812378</td>\n",
       "      <td>0.790533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>MeanAbsolutePercentageError_LogCoshError</td>\n",
       "      <td>9.689545</td>\n",
       "      <td>13.907456</td>\n",
       "      <td>1.612969</td>\n",
       "      <td>2.189927</td>\n",
       "      <td>0.829799</td>\n",
       "      <td>0.788785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>MeanAbsolutePercentageError_KLDivergence</td>\n",
       "      <td>10.751754</td>\n",
       "      <td>14.997637</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.820966</td>\n",
       "      <td>0.788698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>MeanAbsoluteError_KLDivergence</td>\n",
       "      <td>2.292174</td>\n",
       "      <td>2.860347</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.827077</td>\n",
       "      <td>0.788020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>MeanAbsoluteError_CosineSimilarity</td>\n",
       "      <td>2.400139</td>\n",
       "      <td>3.074118</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.820369</td>\n",
       "      <td>0.787087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>MeanAbsolutePercentageError_CosineSimilarity</td>\n",
       "      <td>10.637877</td>\n",
       "      <td>15.322893</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.802662</td>\n",
       "      <td>0.785564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogCosh_MeanAbsoluteError</td>\n",
       "      <td>1.638067</td>\n",
       "      <td>2.291421</td>\n",
       "      <td>2.172313</td>\n",
       "      <td>2.869664</td>\n",
       "      <td>0.858160</td>\n",
       "      <td>0.782960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>MeanAbsoluteError_MeanAbsolutePercentageError</td>\n",
       "      <td>2.197027</td>\n",
       "      <td>2.913420</td>\n",
       "      <td>9.969976</td>\n",
       "      <td>14.610860</td>\n",
       "      <td>0.836132</td>\n",
       "      <td>0.782232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Huber_Hinge</td>\n",
       "      <td>1.788935</td>\n",
       "      <td>2.489674</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.847274</td>\n",
       "      <td>0.781944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>MeanSquaredLogarithmicError_CosineSimilarity</td>\n",
       "      <td>0.023004</td>\n",
       "      <td>0.038939</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.826480</td>\n",
       "      <td>0.781780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>MeanSquaredLogarithmicError_SquaredHinge</td>\n",
       "      <td>0.020969</td>\n",
       "      <td>0.036704</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.849373</td>\n",
       "      <td>0.780268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>LogCosh_Poisson</td>\n",
       "      <td>1.597593</td>\n",
       "      <td>2.250706</td>\n",
       "      <td>-48.754101</td>\n",
       "      <td>-50.737888</td>\n",
       "      <td>0.862803</td>\n",
       "      <td>0.780133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  train_loss  test_loss  \\\n",
       "48     MeanAbsolutePercentageError_MeanAbsoluteError   10.301604  14.314724   \n",
       "53  MeanAbsolutePercentageError_RootMeanSquaredError   10.632647  14.233661   \n",
       "28                          LogCosh_MeanSquaredError    1.625854   2.236084   \n",
       "7                  Huber_MeanSquaredLogarithmicError    1.902591   2.431320   \n",
       "34                           MeanAbsoluteError_Hinge    2.141349   2.789681   \n",
       "52               MeanAbsolutePercentageError_Poisson   10.079053  14.431956   \n",
       "27               LogCosh_MeanAbsolutePercentageError    1.731798   2.328740   \n",
       "31                      LogCosh_RootMeanSquaredError    1.628946   2.250333   \n",
       "50      MeanAbsolutePercentageError_MeanSquaredError   10.322030  13.745417   \n",
       "47          MeanAbsolutePercentageError_LogCoshError    9.689545  13.907456   \n",
       "46          MeanAbsolutePercentageError_KLDivergence   10.751754  14.997637   \n",
       "35                    MeanAbsoluteError_KLDivergence    2.292174   2.860347   \n",
       "33                MeanAbsoluteError_CosineSimilarity    2.400139   3.074118   \n",
       "44      MeanAbsolutePercentageError_CosineSimilarity   10.637877  15.322893   \n",
       "26                         LogCosh_MeanAbsoluteError    1.638067   2.291421   \n",
       "38     MeanAbsoluteError_MeanAbsolutePercentageError    2.197027   2.913420   \n",
       "1                                        Huber_Hinge    1.788935   2.489674   \n",
       "66      MeanSquaredLogarithmicError_CosineSimilarity    0.023004   0.038939   \n",
       "76          MeanSquaredLogarithmicError_SquaredHinge    0.020969   0.036704   \n",
       "30                                   LogCosh_Poisson    1.597593   2.250706   \n",
       "\n",
       "    train_metric  test_metric  train_r2   test_r2  \n",
       "48      2.266057     2.812323  0.818855  0.812048  \n",
       "53      3.726139     4.047540  0.835928  0.803198  \n",
       "28     12.221090    16.542501  0.855581  0.801277  \n",
       "7       0.023448     0.040783  0.821507  0.800495  \n",
       "34      0.000000     0.000000  0.837433  0.799163  \n",
       "52    -48.682350   -50.752823  0.817363  0.797132  \n",
       "27     10.765066    15.140324  0.834882  0.791863  \n",
       "31      3.492333     4.168153  0.855873  0.791294  \n",
       "50     15.876963    17.436861  0.812378  0.790533  \n",
       "47      1.612969     2.189927  0.829799  0.788785  \n",
       "46      0.000000     0.000000  0.820966  0.788698  \n",
       "35      0.000000     0.000000  0.827077  0.788020  \n",
       "33      1.000000     1.000000  0.820369  0.787087  \n",
       "44      1.000000     1.000000  0.802662  0.785564  \n",
       "26      2.172313     2.869664  0.858160  0.782960  \n",
       "38      9.969976    14.610860  0.836132  0.782232  \n",
       "1       0.000000     0.000000  0.847274  0.781944  \n",
       "66      1.000000     1.000000  0.826480  0.781780  \n",
       "76      0.000000     0.000000  0.849373  0.780268  \n",
       "30    -48.754101   -50.737888  0.862803  0.780133  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values(by=['test_r2'], ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa69d263",
   "metadata": {},
   "source": [
    "Похоже, что что-то они могут)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72e4b84",
   "metadata": {},
   "source": [
    "Выберем первую строку и попытемся подобрать слои и нейроны"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d349d042",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.MeanAbsolutePercentageError\n",
    "metric = tf.keras.metrics.MeanAbsoluteError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8257ccab",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_neurons = [50, 100, 200, 500]\n",
    "layers = list(range(1, 6))\n",
    "models_2 = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6554737",
   "metadata": {},
   "source": [
    "Немного уточним дельту"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "096774c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(\n",
    "                              monitor=\"val_loss\",\n",
    "                              min_delta=0.00001,\n",
    "                              patience=3,\n",
    "                              verbose=1,\n",
    "                              mode=\"auto\",\n",
    "                              baseline=None,\n",
    "                              restore_best_weights=True,\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6024163f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00051: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 9.5652 - mean_absolute_error: 2.1918\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 14.1252 - mean_absolute_error: 2.8417\n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00029: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 10.1584 - mean_absolute_error: 2.3413\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 14.2167 - mean_absolute_error: 2.9360\n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 10.9266 - mean_absolute_error: 2.4522\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 15.4370 - mean_absolute_error: 3.1364\n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 10.6862 - mean_absolute_error: 2.3270\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 15.0094 - mean_absolute_error: 3.0110\n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00025: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 10.8363 - mean_absolute_error: 2.3839\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 15.4781 - mean_absolute_error: 3.1081\n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00026: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 9.9910 - mean_absolute_error: 2.3483\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 14.7649 - mean_absolute_error: 3.1514\n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 10.9915 - mean_absolute_error: 2.4657\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 14.8188 - mean_absolute_error: 3.0954\n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 10.9012 - mean_absolute_error: 2.4161\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 15.4507 - mean_absolute_error: 3.0894\n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00025: early stopping\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 10.4925 - mean_absolute_error: 2.3219\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 13.9127 - mean_absolute_error: 2.8006\n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00020: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 9.9521 - mean_absolute_error: 2.2552\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 14.4128 - mean_absolute_error: 3.0489\n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 11.7385 - mean_absolute_error: 2.4318\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 16.4741 - mean_absolute_error: 3.1317\n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 12.9265 - mean_absolute_error: 2.9090\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 17.9571 - mean_absolute_error: 3.7028\n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00031: early stopping\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 10.4372 - mean_absolute_error: 2.2924\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 14.7292 - mean_absolute_error: 2.9203\n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 11.5060 - mean_absolute_error: 2.5620\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 16.9199 - mean_absolute_error: 3.4793\n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 11.0284 - mean_absolute_error: 2.4989\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 15.6521 - mean_absolute_error: 3.2284\n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 11.4625 - mean_absolute_error: 2.4352\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 15.9289 - mean_absolute_error: 3.0620\n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00026: early stopping\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 10.9545 - mean_absolute_error: 2.4807\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 15.2175 - mean_absolute_error: 3.1040\n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 12.3939 - mean_absolute_error: 2.9088\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 16.5135 - mean_absolute_error: 3.5927\n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 10.8673 - mean_absolute_error: 2.3715\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 14.9515 - mean_absolute_error: 3.0109\n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 10.4522 - mean_absolute_error: 2.3608\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 15.2097 - mean_absolute_error: 3.2494\n",
      "\n",
      "CPU times: total: 1min 13s\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "for layer in layers:\n",
    "    for start in start_neurons:\n",
    "        schema = [start]\n",
    "        for i in range(layer):\n",
    "            schema.append(int(0.7 * schema[-1]))\n",
    "        name = str(schema)\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Dense(start, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "        for i in schema[1:]:\n",
    "            model.add(Dense(i, activation='relu'))\n",
    "        model.add(Dense(1))\n",
    "        \n",
    "        model.compile(\n",
    "                optimizer='adam',\n",
    "                loss=[loss()],\n",
    "                metrics=[metric()]\n",
    "            )\n",
    "        history = model.fit(X_train, y_train,\n",
    "                      epochs=10000,\n",
    "                      batch_size=20, \n",
    "                      validation_split=0.2,\n",
    "                      verbose=0,\n",
    "                      callbacks=[callback],\n",
    "                  )\n",
    "        train_metrics = model.evaluate(X_train, y_train)\n",
    "        test_metrics = model.evaluate(X_test, y_test)\n",
    "        pred_train = model.predict(X_train)\n",
    "        train_r2 = r2_score(y_train, pred_train)\n",
    "        pred_test = model.predict(X_test)\n",
    "        test_r2 = r2_score(y_test, pred_test)\n",
    "\n",
    "        models_2.append({\n",
    "            'name': name,\n",
    "            'model': model,\n",
    "            'history': history,\n",
    "            'train_loss': train_metrics[0],\n",
    "            'test_loss': test_metrics[0],\n",
    "            'train_metric': train_metrics[1],\n",
    "            'test_metric': test_metrics[1],\n",
    "            'train_r2': train_r2,\n",
    "            'test_r2': test_r2,\n",
    "        })\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89157908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>train_metric</th>\n",
       "      <th>test_metric</th>\n",
       "      <th>train_r2</th>\n",
       "      <th>test_r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[50, 35]</td>\n",
       "      <td>9.565228</td>\n",
       "      <td>14.125244</td>\n",
       "      <td>2.191763</td>\n",
       "      <td>2.841683</td>\n",
       "      <td>0.807875</td>\n",
       "      <td>0.808256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[100, 70]</td>\n",
       "      <td>10.158446</td>\n",
       "      <td>14.216733</td>\n",
       "      <td>2.341311</td>\n",
       "      <td>2.936005</td>\n",
       "      <td>0.774937</td>\n",
       "      <td>0.791986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[200, 140]</td>\n",
       "      <td>10.926607</td>\n",
       "      <td>15.437003</td>\n",
       "      <td>2.452211</td>\n",
       "      <td>3.136356</td>\n",
       "      <td>0.800460</td>\n",
       "      <td>0.766523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[500, 350]</td>\n",
       "      <td>10.686156</td>\n",
       "      <td>15.009403</td>\n",
       "      <td>2.326963</td>\n",
       "      <td>3.011032</td>\n",
       "      <td>0.825322</td>\n",
       "      <td>0.784842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[50, 35, 24]</td>\n",
       "      <td>10.836260</td>\n",
       "      <td>15.478106</td>\n",
       "      <td>2.383885</td>\n",
       "      <td>3.108092</td>\n",
       "      <td>0.811500</td>\n",
       "      <td>0.771000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[100, 70, 49]</td>\n",
       "      <td>9.990983</td>\n",
       "      <td>14.764905</td>\n",
       "      <td>2.348269</td>\n",
       "      <td>3.151393</td>\n",
       "      <td>0.787439</td>\n",
       "      <td>0.765578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[200, 140, 98]</td>\n",
       "      <td>10.991488</td>\n",
       "      <td>14.818805</td>\n",
       "      <td>2.465712</td>\n",
       "      <td>3.095351</td>\n",
       "      <td>0.798755</td>\n",
       "      <td>0.777826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[500, 350, 244]</td>\n",
       "      <td>10.901162</td>\n",
       "      <td>15.450741</td>\n",
       "      <td>2.416084</td>\n",
       "      <td>3.089428</td>\n",
       "      <td>0.804855</td>\n",
       "      <td>0.769593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[50, 35, 24, 16]</td>\n",
       "      <td>10.492477</td>\n",
       "      <td>13.912704</td>\n",
       "      <td>2.321939</td>\n",
       "      <td>2.800629</td>\n",
       "      <td>0.811583</td>\n",
       "      <td>0.796630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[100, 70, 49, 34]</td>\n",
       "      <td>9.952058</td>\n",
       "      <td>14.412814</td>\n",
       "      <td>2.255174</td>\n",
       "      <td>3.048877</td>\n",
       "      <td>0.817775</td>\n",
       "      <td>0.772127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[200, 140, 98, 68]</td>\n",
       "      <td>11.738538</td>\n",
       "      <td>16.474117</td>\n",
       "      <td>2.431751</td>\n",
       "      <td>3.131738</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.769878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[500, 350, 244, 170]</td>\n",
       "      <td>12.926451</td>\n",
       "      <td>17.957071</td>\n",
       "      <td>2.909037</td>\n",
       "      <td>3.702799</td>\n",
       "      <td>0.768410</td>\n",
       "      <td>0.709298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[50, 35, 24, 16, 11]</td>\n",
       "      <td>10.437220</td>\n",
       "      <td>14.729230</td>\n",
       "      <td>2.292365</td>\n",
       "      <td>2.920325</td>\n",
       "      <td>0.815367</td>\n",
       "      <td>0.814198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[100, 70, 49, 34, 23]</td>\n",
       "      <td>11.505954</td>\n",
       "      <td>16.919928</td>\n",
       "      <td>2.562028</td>\n",
       "      <td>3.479277</td>\n",
       "      <td>0.802801</td>\n",
       "      <td>0.728791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[200, 140, 98, 68, 47]</td>\n",
       "      <td>11.028448</td>\n",
       "      <td>15.652143</td>\n",
       "      <td>2.498917</td>\n",
       "      <td>3.228354</td>\n",
       "      <td>0.790430</td>\n",
       "      <td>0.754331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[500, 350, 244, 170, 118]</td>\n",
       "      <td>11.462453</td>\n",
       "      <td>15.928935</td>\n",
       "      <td>2.435151</td>\n",
       "      <td>3.061995</td>\n",
       "      <td>0.828477</td>\n",
       "      <td>0.769544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[50, 35, 24, 16, 11, 7]</td>\n",
       "      <td>10.954453</td>\n",
       "      <td>15.217505</td>\n",
       "      <td>2.480672</td>\n",
       "      <td>3.103962</td>\n",
       "      <td>0.779832</td>\n",
       "      <td>0.774624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[100, 70, 49, 34, 23, 16]</td>\n",
       "      <td>12.393909</td>\n",
       "      <td>16.513510</td>\n",
       "      <td>2.908786</td>\n",
       "      <td>3.592660</td>\n",
       "      <td>0.729307</td>\n",
       "      <td>0.723355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[200, 140, 98, 68, 47, 32]</td>\n",
       "      <td>10.867265</td>\n",
       "      <td>14.951520</td>\n",
       "      <td>2.371538</td>\n",
       "      <td>3.010934</td>\n",
       "      <td>0.819357</td>\n",
       "      <td>0.782751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[500, 350, 244, 170, 118, 82]</td>\n",
       "      <td>10.452204</td>\n",
       "      <td>15.209698</td>\n",
       "      <td>2.360762</td>\n",
       "      <td>3.249378</td>\n",
       "      <td>0.824265</td>\n",
       "      <td>0.748071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             name  train_loss  test_loss  train_metric  \\\n",
       "0                        [50, 35]    9.565228  14.125244      2.191763   \n",
       "1                       [100, 70]   10.158446  14.216733      2.341311   \n",
       "2                      [200, 140]   10.926607  15.437003      2.452211   \n",
       "3                      [500, 350]   10.686156  15.009403      2.326963   \n",
       "4                    [50, 35, 24]   10.836260  15.478106      2.383885   \n",
       "5                   [100, 70, 49]    9.990983  14.764905      2.348269   \n",
       "6                  [200, 140, 98]   10.991488  14.818805      2.465712   \n",
       "7                 [500, 350, 244]   10.901162  15.450741      2.416084   \n",
       "8                [50, 35, 24, 16]   10.492477  13.912704      2.321939   \n",
       "9               [100, 70, 49, 34]    9.952058  14.412814      2.255174   \n",
       "10             [200, 140, 98, 68]   11.738538  16.474117      2.431751   \n",
       "11           [500, 350, 244, 170]   12.926451  17.957071      2.909037   \n",
       "12           [50, 35, 24, 16, 11]   10.437220  14.729230      2.292365   \n",
       "13          [100, 70, 49, 34, 23]   11.505954  16.919928      2.562028   \n",
       "14         [200, 140, 98, 68, 47]   11.028448  15.652143      2.498917   \n",
       "15      [500, 350, 244, 170, 118]   11.462453  15.928935      2.435151   \n",
       "16        [50, 35, 24, 16, 11, 7]   10.954453  15.217505      2.480672   \n",
       "17      [100, 70, 49, 34, 23, 16]   12.393909  16.513510      2.908786   \n",
       "18     [200, 140, 98, 68, 47, 32]   10.867265  14.951520      2.371538   \n",
       "19  [500, 350, 244, 170, 118, 82]   10.452204  15.209698      2.360762   \n",
       "\n",
       "    test_metric  train_r2   test_r2  \n",
       "0      2.841683  0.807875  0.808256  \n",
       "1      2.936005  0.774937  0.791986  \n",
       "2      3.136356  0.800460  0.766523  \n",
       "3      3.011032  0.825322  0.784842  \n",
       "4      3.108092  0.811500  0.771000  \n",
       "5      3.151393  0.787439  0.765578  \n",
       "6      3.095351  0.798755  0.777826  \n",
       "7      3.089428  0.804855  0.769593  \n",
       "8      2.800629  0.811583  0.796630  \n",
       "9      3.048877  0.817775  0.772127  \n",
       "10     3.131738  0.829882  0.769878  \n",
       "11     3.702799  0.768410  0.709298  \n",
       "12     2.920325  0.815367  0.814198  \n",
       "13     3.479277  0.802801  0.728791  \n",
       "14     3.228354  0.790430  0.754331  \n",
       "15     3.061995  0.828477  0.769544  \n",
       "16     3.103962  0.779832  0.774624  \n",
       "17     3.592660  0.729307  0.723355  \n",
       "18     3.010934  0.819357  0.782751  \n",
       "19     3.249378  0.824265  0.748071  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_2 = pd.DataFrame(models_2).drop(['model', 'history'],axis=1)\n",
    "results_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d5e2960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>train_metric</th>\n",
       "      <th>test_metric</th>\n",
       "      <th>train_r2</th>\n",
       "      <th>test_r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[50, 35, 24, 16, 11]</td>\n",
       "      <td>10.437220</td>\n",
       "      <td>14.729230</td>\n",
       "      <td>2.292365</td>\n",
       "      <td>2.920325</td>\n",
       "      <td>0.815367</td>\n",
       "      <td>0.814198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[50, 35]</td>\n",
       "      <td>9.565228</td>\n",
       "      <td>14.125244</td>\n",
       "      <td>2.191763</td>\n",
       "      <td>2.841683</td>\n",
       "      <td>0.807875</td>\n",
       "      <td>0.808256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[50, 35, 24, 16]</td>\n",
       "      <td>10.492477</td>\n",
       "      <td>13.912704</td>\n",
       "      <td>2.321939</td>\n",
       "      <td>2.800629</td>\n",
       "      <td>0.811583</td>\n",
       "      <td>0.796630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[100, 70]</td>\n",
       "      <td>10.158446</td>\n",
       "      <td>14.216733</td>\n",
       "      <td>2.341311</td>\n",
       "      <td>2.936005</td>\n",
       "      <td>0.774937</td>\n",
       "      <td>0.791986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[500, 350]</td>\n",
       "      <td>10.686156</td>\n",
       "      <td>15.009403</td>\n",
       "      <td>2.326963</td>\n",
       "      <td>3.011032</td>\n",
       "      <td>0.825322</td>\n",
       "      <td>0.784842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[200, 140, 98, 68, 47, 32]</td>\n",
       "      <td>10.867265</td>\n",
       "      <td>14.951520</td>\n",
       "      <td>2.371538</td>\n",
       "      <td>3.010934</td>\n",
       "      <td>0.819357</td>\n",
       "      <td>0.782751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[200, 140, 98]</td>\n",
       "      <td>10.991488</td>\n",
       "      <td>14.818805</td>\n",
       "      <td>2.465712</td>\n",
       "      <td>3.095351</td>\n",
       "      <td>0.798755</td>\n",
       "      <td>0.777826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[50, 35, 24, 16, 11, 7]</td>\n",
       "      <td>10.954453</td>\n",
       "      <td>15.217505</td>\n",
       "      <td>2.480672</td>\n",
       "      <td>3.103962</td>\n",
       "      <td>0.779832</td>\n",
       "      <td>0.774624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[100, 70, 49, 34]</td>\n",
       "      <td>9.952058</td>\n",
       "      <td>14.412814</td>\n",
       "      <td>2.255174</td>\n",
       "      <td>3.048877</td>\n",
       "      <td>0.817775</td>\n",
       "      <td>0.772127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[50, 35, 24]</td>\n",
       "      <td>10.836260</td>\n",
       "      <td>15.478106</td>\n",
       "      <td>2.383885</td>\n",
       "      <td>3.108092</td>\n",
       "      <td>0.811500</td>\n",
       "      <td>0.771000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[200, 140, 98, 68]</td>\n",
       "      <td>11.738538</td>\n",
       "      <td>16.474117</td>\n",
       "      <td>2.431751</td>\n",
       "      <td>3.131738</td>\n",
       "      <td>0.829882</td>\n",
       "      <td>0.769878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[500, 350, 244]</td>\n",
       "      <td>10.901162</td>\n",
       "      <td>15.450741</td>\n",
       "      <td>2.416084</td>\n",
       "      <td>3.089428</td>\n",
       "      <td>0.804855</td>\n",
       "      <td>0.769593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[500, 350, 244, 170, 118]</td>\n",
       "      <td>11.462453</td>\n",
       "      <td>15.928935</td>\n",
       "      <td>2.435151</td>\n",
       "      <td>3.061995</td>\n",
       "      <td>0.828477</td>\n",
       "      <td>0.769544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[200, 140]</td>\n",
       "      <td>10.926607</td>\n",
       "      <td>15.437003</td>\n",
       "      <td>2.452211</td>\n",
       "      <td>3.136356</td>\n",
       "      <td>0.800460</td>\n",
       "      <td>0.766523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[100, 70, 49]</td>\n",
       "      <td>9.990983</td>\n",
       "      <td>14.764905</td>\n",
       "      <td>2.348269</td>\n",
       "      <td>3.151393</td>\n",
       "      <td>0.787439</td>\n",
       "      <td>0.765578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[200, 140, 98, 68, 47]</td>\n",
       "      <td>11.028448</td>\n",
       "      <td>15.652143</td>\n",
       "      <td>2.498917</td>\n",
       "      <td>3.228354</td>\n",
       "      <td>0.790430</td>\n",
       "      <td>0.754331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[500, 350, 244, 170, 118, 82]</td>\n",
       "      <td>10.452204</td>\n",
       "      <td>15.209698</td>\n",
       "      <td>2.360762</td>\n",
       "      <td>3.249378</td>\n",
       "      <td>0.824265</td>\n",
       "      <td>0.748071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[100, 70, 49, 34, 23]</td>\n",
       "      <td>11.505954</td>\n",
       "      <td>16.919928</td>\n",
       "      <td>2.562028</td>\n",
       "      <td>3.479277</td>\n",
       "      <td>0.802801</td>\n",
       "      <td>0.728791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[100, 70, 49, 34, 23, 16]</td>\n",
       "      <td>12.393909</td>\n",
       "      <td>16.513510</td>\n",
       "      <td>2.908786</td>\n",
       "      <td>3.592660</td>\n",
       "      <td>0.729307</td>\n",
       "      <td>0.723355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[500, 350, 244, 170]</td>\n",
       "      <td>12.926451</td>\n",
       "      <td>17.957071</td>\n",
       "      <td>2.909037</td>\n",
       "      <td>3.702799</td>\n",
       "      <td>0.768410</td>\n",
       "      <td>0.709298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             name  train_loss  test_loss  train_metric  \\\n",
       "12           [50, 35, 24, 16, 11]   10.437220  14.729230      2.292365   \n",
       "0                        [50, 35]    9.565228  14.125244      2.191763   \n",
       "8                [50, 35, 24, 16]   10.492477  13.912704      2.321939   \n",
       "1                       [100, 70]   10.158446  14.216733      2.341311   \n",
       "3                      [500, 350]   10.686156  15.009403      2.326963   \n",
       "18     [200, 140, 98, 68, 47, 32]   10.867265  14.951520      2.371538   \n",
       "6                  [200, 140, 98]   10.991488  14.818805      2.465712   \n",
       "16        [50, 35, 24, 16, 11, 7]   10.954453  15.217505      2.480672   \n",
       "9               [100, 70, 49, 34]    9.952058  14.412814      2.255174   \n",
       "4                    [50, 35, 24]   10.836260  15.478106      2.383885   \n",
       "10             [200, 140, 98, 68]   11.738538  16.474117      2.431751   \n",
       "7                 [500, 350, 244]   10.901162  15.450741      2.416084   \n",
       "15      [500, 350, 244, 170, 118]   11.462453  15.928935      2.435151   \n",
       "2                      [200, 140]   10.926607  15.437003      2.452211   \n",
       "5                   [100, 70, 49]    9.990983  14.764905      2.348269   \n",
       "14         [200, 140, 98, 68, 47]   11.028448  15.652143      2.498917   \n",
       "19  [500, 350, 244, 170, 118, 82]   10.452204  15.209698      2.360762   \n",
       "13          [100, 70, 49, 34, 23]   11.505954  16.919928      2.562028   \n",
       "17      [100, 70, 49, 34, 23, 16]   12.393909  16.513510      2.908786   \n",
       "11           [500, 350, 244, 170]   12.926451  17.957071      2.909037   \n",
       "\n",
       "    test_metric  train_r2   test_r2  \n",
       "12     2.920325  0.815367  0.814198  \n",
       "0      2.841683  0.807875  0.808256  \n",
       "8      2.800629  0.811583  0.796630  \n",
       "1      2.936005  0.774937  0.791986  \n",
       "3      3.011032  0.825322  0.784842  \n",
       "18     3.010934  0.819357  0.782751  \n",
       "6      3.095351  0.798755  0.777826  \n",
       "16     3.103962  0.779832  0.774624  \n",
       "9      3.048877  0.817775  0.772127  \n",
       "4      3.108092  0.811500  0.771000  \n",
       "10     3.131738  0.829882  0.769878  \n",
       "7      3.089428  0.804855  0.769593  \n",
       "15     3.061995  0.828477  0.769544  \n",
       "2      3.136356  0.800460  0.766523  \n",
       "5      3.151393  0.787439  0.765578  \n",
       "14     3.228354  0.790430  0.754331  \n",
       "19     3.249378  0.824265  0.748071  \n",
       "13     3.479277  0.802801  0.728791  \n",
       "17     3.592660  0.729307  0.723355  \n",
       "11     3.702799  0.768410  0.709298  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_2.sort_values(by=['test_r2'], ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92678c86",
   "metadata": {},
   "source": [
    "Похоже, что такого подбора параметров недостаточно, так как топ слишком разнообразен, чтобы сделать вывод."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d8bcf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
